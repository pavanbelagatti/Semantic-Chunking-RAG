{"cells":[{"attachments":{},"cell_type":"markdown","id":"aff4d01d-c210-49e2-8c6d-b72a2c68019f","metadata":{"language":"python"},"source":"# Semantic Chunking for RAG"},{"attachments":{},"cell_type":"markdown","id":"5dee55cd-cb44-4fa8-a041-2a5ae6d6a545","metadata":{"language":"python"},"source":"LLMs are bound to hallucinate and then we have different strategies to mitigate this hallucination behaviour of LLMs. One such strategy is Retrieval Augmented Generation (RAG), where a knowledge base is already augmented/provided to the LLM to retrieve the information from and this way LLMs won't hallucinate since the knowledge base is already specified. \n\nRAG involves a step by step process of loading the documents/data, splitting the documents into chunks using any AI framework such as LangChain or LlamaIndex, and vector embeddings generation for the data and storing these embeddings in a vector database.\n\nSo, broadly we can devide the RAG into two main parts, Storing and Retrieval.\n\nWhile enahncing our RAG pipeline, one thing we need to looak at is the retriavl strategy and technoques involved.\nWe can improve retrieval in RAG using the proper chunking strategy. But finding the right chunk size for any given text is a very hard question in general.\n\nToday, we will see how semantic chunking works.\n\nSemantic Chunking considers the relationships within the text. It divides the text into meaningful, semantically complete chunks. This approach ensures the information’s integrity during retrieval, leading to a more accurate and contextually appropriate outcome.\n\nLet's experiment with Semantic chunking & see the results"},{"attachments":{},"cell_type":"markdown","id":"f65d2e27-8644-4c0d-a71a-47567f2b35df","metadata":{"language":"python"},"source":"## Tech Stack Used\n#### LangChain - Open source AI fraamework to load, split and to create embeddings of the data\n#### SingleStore - As a robust vector database to store vector embeddings\n#### Groq and HuggingFace - To choose our LLMs and embedding models"},{"attachments":{},"cell_type":"markdown","id":"4215e872-ff78-4380-9f1f-0145453a074c","metadata":{"language":"python"},"source":"### Download data"},{"cell_type":"code","execution_count":8,"id":"91ace08f-c9d2-4145-9782-887da314c89d","metadata":{"execution":{"iopub.execute_input":"2024-07-15T05:48:13.697239Z","iopub.status.busy":"2024-07-15T05:48:13.696951Z","iopub.status.idle":"2024-07-15T05:48:14.004703Z","shell.execute_reply":"2024-07-15T05:48:14.004151Z","shell.execute_reply.started":"2024-07-15T05:48:13.697219Z"},"language":"python","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"--2024-07-15 05:48:13--  https://arxiv.org/pdf/1810.04805.pdf\nResolving arxiv.org (arxiv.org)... 151.101.67.42, 151.101.131.42, 151.101.195.42, ...\nConnecting to arxiv.org (arxiv.org)|151.101.67.42|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: http://arxiv.org/pdf/1810.04805 [following]\n--2024-07-15 05:48:13--  http://arxiv.org/pdf/1810.04805\nConnecting to arxiv.org (arxiv.org)|151.101.67.42|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 775166 (757K) [application/pdf]\nSaving to: ‘1810.04805.pdf.1’\n\n1810.04805.pdf.1    100%[===================>] 757.00K  --.-KB/s    in 0.02s   \n\n2024-07-15 05:48:13 (41.3 MB/s) - ‘1810.04805.pdf.1’ saved [775166/775166]\n\n"}],"source":"! wget \"https://arxiv.org/pdf/1810.04805.pdf\""},{"attachments":{},"cell_type":"markdown","id":"8956ff12-a8b6-4b6d-95a7-27387b816e75","metadata":{"language":"python"},"source":"## Process the PDF Content"},{"cell_type":"code","execution_count":9,"id":"05a6c2db-47fd-4c4d-8fa8-574ef97258ea","metadata":{"execution":{"iopub.execute_input":"2024-07-15T05:48:36.091281Z","iopub.status.busy":"2024-07-15T05:48:36.091013Z","iopub.status.idle":"2024-07-15T05:48:36.314739Z","shell.execute_reply":"2024-07-15T05:48:36.314245Z","shell.execute_reply.started":"2024-07-15T05:48:36.091263Z"},"language":"python","trusted":true},"outputs":[],"source":"from langchain.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter"},{"cell_type":"code","execution_count":10,"id":"9f2d9650-d329-43aa-a760-2ac5e4dfb9a7","metadata":{"execution":{"iopub.execute_input":"2024-07-15T05:48:38.883549Z","iopub.status.busy":"2024-07-15T05:48:38.883311Z","iopub.status.idle":"2024-07-15T05:48:39.423312Z","shell.execute_reply":"2024-07-15T05:48:39.422872Z","shell.execute_reply.started":"2024-07-15T05:48:38.883534Z"},"language":"python","trusted":true},"outputs":[],"source":"loader = PyPDFLoader(\"1810.04805.pdf\")\ndocuments = loader.load()"},{"cell_type":"code","execution_count":11,"id":"a1b77f9a-d34d-47d1-865a-39f602f57dd7","metadata":{"execution":{"iopub.execute_input":"2024-07-15T05:48:51.465471Z","iopub.status.busy":"2024-07-15T05:48:51.465220Z","iopub.status.idle":"2024-07-15T05:48:51.468425Z","shell.execute_reply":"2024-07-15T05:48:51.467876Z","shell.execute_reply.started":"2024-07-15T05:48:51.465455Z"},"language":"python","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"16\n"}],"source":"print(len(documents))"},{"attachments":{},"cell_type":"markdown","id":"be954d6c-5727-407b-8250-7e0610074a8d","metadata":{"language":"python"},"source":"## Perform Naive Chunking(RecursiveCharacterTextSplitting)"},{"cell_type":"code","execution_count":12,"id":"8b59f01a-a932-4c47-b742-300f93431fa8","metadata":{"execution":{"iopub.execute_input":"2024-07-15T05:49:05.291864Z","iopub.status.busy":"2024-07-15T05:49:05.291624Z","iopub.status.idle":"2024-07-15T05:49:05.294641Z","shell.execute_reply":"2024-07-15T05:49:05.294129Z","shell.execute_reply.started":"2024-07-15T05:49:05.291850Z"},"language":"python","trusted":true},"outputs":[],"source":"from langchain.text_splitter import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=0,\n    length_function=len,\n    is_separator_regex=False\n)"},{"cell_type":"code","execution_count":13,"id":"048c72de-4029-4f1f-bb13-dd6bc8ece64d","metadata":{"execution":{"iopub.execute_input":"2024-07-15T05:49:14.760569Z","iopub.status.busy":"2024-07-15T05:49:14.760278Z","iopub.status.idle":"2024-07-15T05:49:14.765578Z","shell.execute_reply":"2024-07-15T05:49:14.765082Z","shell.execute_reply.started":"2024-07-15T05:49:14.760551Z"},"language":"python","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"BERT BERT \nE[CLS] E1 E[SEP] ... ENE1’... EM’\nC\nT1\nT[SEP] ...\n TN\nT1’...\n TM’\n[CLS] Tok 1 [SEP] ... Tok NTok 1 ... TokM \nQuestion Paragraph Start/End Span \nBERT \nE[CLS] E1 E[SEP] ... ENE1’... EM’\nC\nT1\nT[SEP] ...\n TN\nT1’...\n TM’\n[CLS] Tok 1 [SEP] ... Tok NTok 1 ... TokM \nMasked Sentence A Masked Sentence B \nPre-training Fine-Tuning NSP Mask LM Mask LM \nUnlabeled Sentence A and B Pair SQuAD \nQuestion Answer Pair NER MNLI Figure 1: Overall pre-training and ﬁne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and ﬁne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During ﬁne-tuning, all parameters are ﬁne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n\n2018; Radford et al., 2018; Dai and Le, 2015).\n2.3 Transfer Learning from Supervised Data\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to ﬁne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n3 BERT\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and ﬁne-tuning . Dur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks. For ﬁne-\ntuning, the BERT model is ﬁrst initialized with\nthe pre-trained parameters, and all of the param-\neters are ﬁne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\n\narate ﬁne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\nA distinctive feature of BERT is its uniﬁed ar-\nchitecture across different tasks. There is mini-mal difference between the pre-trained architec-\nture and the ﬁnal downstream architecture.\nModel Architecture BERT’s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthetensor2tensor library.1Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as “The Annotated Transformer.”2\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\n\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERT BASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERT LARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\nBERT BASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n1https://github.com/tensorﬂow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/ﬁlter size to be 4H,\ni.e., 3072 for the H= 768 and 4096 for the H= 1024 .\n4We note that in the literature the bidirectional Trans-\n\nInput/Output Representations To make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g.,⟨Question, Answer⟩) in one token sequence.\nThroughout this work, a “sentence” can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A “sequence” refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The ﬁrst\ntoken of every sequence is always a special clas-\nsiﬁcation token ( [CLS] ). The ﬁnal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classiﬁcation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ( [SEP] ). Second, we add a learned embed-\n\n"}],"source":"naive_chunks = text_splitter.split_documents(documents)\nfor chunk in naive_chunks[10:15]:\n  print(chunk.page_content+ \"\\n\")"},{"attachments":{},"cell_type":"markdown","id":"04442de4-0414-4a7b-911e-94ab7cbfdaf3","metadata":{"language":"python"},"source":"## Instantiate Embedding Model"},{"cell_type":"code","execution_count":33,"id":"9e26fce9-057b-4ec5-9e09-ca5a8df146b3","metadata":{"execution":{"iopub.execute_input":"2024-07-15T05:57:30.214056Z","iopub.status.busy":"2024-07-15T05:57:30.213782Z","iopub.status.idle":"2024-07-15T05:59:12.188609Z","shell.execute_reply":"2024-07-15T05:59:12.187928Z","shell.execute_reply.started":"2024-07-15T05:57:30.214041Z"},"language":"python","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Note: you may need to restart the kernel to use updated packages.\n"}],"source":"pip install sentence-transformers --quiet"},{"cell_type":"code","execution_count":38,"id":"6df7abcf-dc5e-446d-a382-2705d867785a","metadata":{"execution":{"iopub.execute_input":"2024-07-15T05:59:18.912722Z","iopub.status.busy":"2024-07-15T05:59:18.912053Z","iopub.status.idle":"2024-07-15T05:59:23.866947Z","shell.execute_reply":"2024-07-15T05:59:23.866297Z","shell.execute_reply.started":"2024-07-15T05:59:18.912696Z"},"language":"python","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":"/opt/conda/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm, trange\n"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b605fa179ade48c58d85543555311f39","version_major":2,"version_minor":0},"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"83f7aeb0ae294a49b04aafdabec6ab3e","version_major":2,"version_minor":0},"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4543ba2a27b549ffb6f2ddb1eb5ba710","version_major":2,"version_minor":0},"text/plain":"README.md:   0%|          | 0.00/94.6k [00:00<?, ?B/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8b1a63a0b8d2417c8ff4019b6bc9bcb1","version_major":2,"version_minor":0},"text/plain":"sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"83e0056b962949918fdf67bcd4c603ee","version_major":2,"version_minor":0},"text/plain":"config.json:   0%|          | 0.00/777 [00:00<?, ?B/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c4868c23c8c4433b9c52f575f250d10e","version_major":2,"version_minor":0},"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7eb317408f0f440e80ba7405fcf29a70","version_major":2,"version_minor":0},"text/plain":"tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ec4ec1ea17fe4acba57b79e93d106db2","version_major":2,"version_minor":0},"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c947ce346cd34fed822a68063e2ce1fe","version_major":2,"version_minor":0},"text/plain":"tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"28ad549d52dd4dc5839dd42a27c3f4dc","version_major":2,"version_minor":0},"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8dc6a237b29240ff824053941f9e861b","version_major":2,"version_minor":0},"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"},"metadata":{},"output_type":"display_data"}],"source":"from langchain.embeddings import HuggingFaceEmbeddings\n\nembed_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")"},{"attachments":{},"cell_type":"markdown","id":"b6bd69b9-340b-4485-ad1f-9070e5c8599e","metadata":{"language":"python"},"source":"### Setup the API Key for LLM"},{"cell_type":"code","execution_count":43,"id":"4b7c2750-68a7-426d-b47b-82309f42a089","metadata":{"execution":{"iopub.execute_input":"2024-07-15T06:00:28.456166Z","iopub.status.busy":"2024-07-15T06:00:28.455906Z","iopub.status.idle":"2024-07-15T06:00:29.039939Z","shell.execute_reply":"2024-07-15T06:00:29.039449Z","shell.execute_reply.started":"2024-07-15T06:00:28.456148Z"},"language":"python","trusted":true},"outputs":[],"source":"from groq import Groq\nfrom langchain_groq import ChatGroq"},{"cell_type":"code","execution_count":44,"id":"1783fc65-e110-4b53-9cc9-28a1b2bb0267","metadata":{"execution":{"iopub.execute_input":"2024-07-15T06:02:01.175993Z","iopub.status.busy":"2024-07-15T06:02:01.175727Z","iopub.status.idle":"2024-07-15T06:02:01.178689Z","shell.execute_reply":"2024-07-15T06:02:01.178128Z","shell.execute_reply.started":"2024-07-15T06:02:01.175976Z"},"language":"python","trusted":true},"outputs":[],"source":"import os\n\ngroq_api_key = os.getenv(\"Add your Groq API Key\")"},{"attachments":{},"cell_type":"markdown","id":"cab9b253-cddd-4ab5-a434-2fd07efef80b","metadata":{"language":"python"},"source":"## Perform Semantic Chunking"},{"cell_type":"code","execution_count":45,"id":"1d54f8a3-0549-4690-9125-c729fead70a4","metadata":{"execution":{"iopub.execute_input":"2024-07-15T06:02:20.658025Z","iopub.status.busy":"2024-07-15T06:02:20.657743Z","iopub.status.idle":"2024-07-15T06:02:20.965644Z","shell.execute_reply":"2024-07-15T06:02:20.965126Z","shell.execute_reply.started":"2024-07-15T06:02:20.658009Z"},"language":"python","trusted":true},"outputs":[],"source":"from langchain_experimental.text_splitter import SemanticChunker\nfrom langchain_openai.embeddings import OpenAIEmbeddings\n\nsemantic_chunker = SemanticChunker(embed_model, breakpoint_threshold_type=\"percentile\")"},{"cell_type":"code","execution_count":46,"id":"6bea3e7c-05ea-4e40-9580-88aa91bc61d3","metadata":{"execution":{"iopub.execute_input":"2024-07-15T06:02:31.838334Z","iopub.status.busy":"2024-07-15T06:02:31.837961Z","iopub.status.idle":"2024-07-15T06:05:00.527872Z","shell.execute_reply":"2024-07-15T06:05:00.527365Z","shell.execute_reply.started":"2024-07-15T06:02:31.838312Z"},"language":"python","trusted":true},"outputs":[],"source":"semantic_chunks = semantic_chunker.create_documents([d.page_content for d in documents])"},{"cell_type":"code","execution_count":51,"id":"5ef4d252-3c5f-4893-9aff-60b956fa3128","metadata":{"execution":{"iopub.execute_input":"2024-07-15T06:05:12.550047Z","iopub.status.busy":"2024-07-15T06:05:12.549775Z","iopub.status.idle":"2024-07-15T06:05:12.553705Z","shell.execute_reply":"2024-07-15T06:05:12.553163Z","shell.execute_reply.started":"2024-07-15T06:05:12.550029Z"},"language":"python","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Dev Set\nTasks MNLI-m QNLI MRPC SST-2 SQuAD\n(Acc) (Acc) (Acc) (Acc) (F1)\nBERT BASE 84.4 88.4 86.7 92.7 88.5\nNo NSP 83.9 84.9 86.5 92.6 87.9\nLTR & No NSP 82.1 84.3 77.5 92.1 77.8\n+ BiLSTM 82.1 84.1 75.7 91.6 84.9\nTable 5: Ablation over the pre-training tasks using the\nBERT BASE architecture. “No NSP” is trained without\nthe next sentence prediction task. “LTR & No NSP” is\ntrained as a left-to-right LM without the next sentence\nprediction, like OpenAI GPT. “+ BiLSTM” adds a ran-\ndomly initialized BiLSTM on top of the “LTR + No\nNSP” model during ﬁne-tuning. ablation studies can be found in Appendix C. 5.1 Effect of Pre-training Tasks\nWe demonstrate the importance of the deep bidi-\nrectionality of BERT by evaluating two pre-\ntraining objectives using exactly the same pre-\ntraining data, ﬁne-tuning scheme, and hyperpa-\nrameters as BERT BASE :\nNo NSP : A bidirectional model which is trained\nusing the “masked LM” (MLM) but without the\n“next sentence prediction” (NSP) task. LTR & No NSP : A left-context-only model which\nis trained using a standard Left-to-Right (LTR)\nLM, rather than an MLM.\n1097\n"}],"source":"for semantic_chunk in semantic_chunks:\n  if \"Effect of Pre-training Tasks\" in semantic_chunk.page_content:\n    print(semantic_chunk.page_content)\n    print(len(semantic_chunk.page_content))"},{"attachments":{},"cell_type":"markdown","id":"da413bb3-daa8-4ebb-8950-a6133065692f","metadata":{"language":"python"},"source":"### Store the chunks in our database"},{"cell_type":"code","execution_count":56,"id":"73dd0f8f-1e64-42e0-a1f9-a5b7c3c13f3c","metadata":{"execution":{"iopub.execute_input":"2024-07-15T06:06:47.936498Z","iopub.status.busy":"2024-07-15T06:06:47.936192Z","iopub.status.idle":"2024-07-15T06:06:50.444677Z","shell.execute_reply":"2024-07-15T06:06:50.444064Z","shell.execute_reply.started":"2024-07-15T06:06:47.936480Z"},"language":"python","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"}],"source":"!pip install singlestoredb --quiet"},{"cell_type":"code","execution_count":57,"id":"c250d37e-21b5-4535-8b18-4e0384ce4bd4","metadata":{"execution":{"iopub.execute_input":"2024-07-15T06:08:24.669502Z","iopub.status.busy":"2024-07-15T06:08:24.669190Z","iopub.status.idle":"2024-07-15T06:08:24.677877Z","shell.execute_reply":"2024-07-15T06:08:24.677342Z","shell.execute_reply.started":"2024-07-15T06:08:24.669482Z"},"language":"python","trusted":true},"outputs":[],"source":"from langchain_community.vectorstores import SingleStoreDB"},{"cell_type":"code","execution_count":77,"id":"e8a5f5b0-9cbe-42af-ad8f-a41d24f4bb99","metadata":{"execution":{"iopub.execute_input":"2024-07-15T06:16:24.066683Z","iopub.status.busy":"2024-07-15T06:16:24.066402Z","iopub.status.idle":"2024-07-15T06:17:01.221675Z","shell.execute_reply":"2024-07-15T06:17:01.221176Z","shell.execute_reply.started":"2024-07-15T06:16:24.066667Z"},"language":"python","trusted":true},"outputs":[],"source":"semantic_chunk_vectorstore = SingleStoreDB.from_documents(semantic_chunks, embedding=embed_model)"},{"attachments":{},"cell_type":"markdown","id":"8c6c0e39-e0ad-43e2-916c-f9a57283c06d","metadata":{"language":"python"},"source":"### Instantiate Retrieval Step"},{"cell_type":"code","execution_count":78,"id":"3e894617-0573-4e7e-a62a-e2c5881467ec","metadata":{"execution":{"iopub.execute_input":"2024-07-15T06:17:22.353671Z","iopub.status.busy":"2024-07-15T06:17:22.353375Z","iopub.status.idle":"2024-07-15T06:17:22.902845Z","shell.execute_reply":"2024-07-15T06:17:22.902248Z","shell.execute_reply.started":"2024-07-15T06:17:22.353649Z"},"language":"python","trusted":true},"outputs":[{"data":{"text/plain":"[Document(page_content='The right part of the paper represents the\\nDev set results. For the feature-based approach,\\nwe concatenate the last 4 layers of BERT as the\\nfeatures, which was shown to be the best approach\\nin Section 5.3. From the table it can be seen that ﬁne-tuning is\\nsurprisingly robust to different masking strategies. However, as expected, using only the M ASK strat-\\negy was problematic when applying the feature-\\nbased approach to NER. Interestingly, using only\\nthe R NDstrategy performs much worse than our\\nstrategy as well.')]"},"execution_count":78,"metadata":{},"output_type":"execute_result"}],"source":"semantic_chunk_retriever = semantic_chunk_vectorstore.as_retriever(search_kwargs={\"k\" : 1})\nsemantic_chunk_retriever.invoke(\"Describe the Feature-based Approach with BERT?\")"},{"attachments":{},"cell_type":"markdown","id":"b55f895b-db88-4766-a96e-e6cbde7a72e6","metadata":{"language":"python"},"source":"### Instantiate Augmentation Step(for content Augmentation)"},{"cell_type":"code","execution_count":84,"id":"5258aa5f-67d3-4777-9090-65cddef24abe","metadata":{"execution":{"iopub.execute_input":"2024-07-15T06:19:22.256949Z","iopub.status.busy":"2024-07-15T06:19:22.256678Z","iopub.status.idle":"2024-07-15T06:19:22.260137Z","shell.execute_reply":"2024-07-15T06:19:22.259601Z","shell.execute_reply.started":"2024-07-15T06:19:22.256931Z"},"language":"python","trusted":true},"outputs":[],"source":"from langchain_core.prompts import ChatPromptTemplate\n\nrag_template = \"\"\"\\\nUse the following context to answer the user's query. If you cannot answer, please respond with 'I don't know'.\n\nUser's Query:\n{question}\n\nContext:\n{context}\n\"\"\"\n\nrag_prompt = ChatPromptTemplate.from_template(rag_template)"},{"attachments":{},"cell_type":"markdown","id":"a0e731b8-b0b6-47e1-8277-7d01396a87cd","metadata":{"language":"python"},"source":"### Instantiate the Generation Step"},{"cell_type":"code","execution_count":88,"id":"412430d2-0b05-4960-8911-ddec5e7347a4","metadata":{"execution":{"iopub.execute_input":"2024-07-15T06:22:53.396831Z","iopub.status.busy":"2024-07-15T06:22:53.396511Z","iopub.status.idle":"2024-07-15T06:22:53.423421Z","shell.execute_reply":"2024-07-15T06:22:53.422865Z","shell.execute_reply.started":"2024-07-15T06:22:53.396808Z"},"language":"python","trusted":true},"outputs":[],"source":"# Define the userdata dictionary with your API key\nuserdata = {\n    \"GROQ_API_KEY\": \"Add your Groq API Key\"\n}\n\n# Initialize the chat model\nchat_model = ChatGroq(\n    temperature=0,\n    model_name=\"mixtral-8x7b-32768\",\n    api_key=userdata.get(\"GROQ_API_KEY\")\n)"},{"attachments":{},"cell_type":"markdown","id":"1355688c-1fbd-4891-94cd-57630b8cabfa","metadata":{"language":"python"},"source":"## RAG Pipeline Utilizing Semantic Chunking"},{"cell_type":"code","execution_count":89,"id":"1ba42342-40b2-4c25-bcad-f4d23e99e818","metadata":{"execution":{"iopub.execute_input":"2024-07-15T06:23:06.419229Z","iopub.status.busy":"2024-07-15T06:23:06.418918Z","iopub.status.idle":"2024-07-15T06:23:06.423308Z","shell.execute_reply":"2024-07-15T06:23:06.422641Z","shell.execute_reply.started":"2024-07-15T06:23:06.419208Z"},"language":"python","trusted":true},"outputs":[],"source":"from langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.output_parsers import StrOutputParser\n\nsemantic_rag_chain = (\n    {\"context\" : semantic_chunk_retriever, \"question\" : RunnablePassthrough()}\n    | rag_prompt\n    | chat_model\n    | StrOutputParser()\n)"},{"attachments":{},"cell_type":"markdown","id":"ba6af9ca-7135-40bb-a2f9-fe3f97b48cc2","metadata":{"language":"python"},"source":"### Ask Question 1"},{"cell_type":"code","execution_count":90,"id":"ed6660a4-19fa-485a-83d9-4c35a9fdf0c3","metadata":{"execution":{"iopub.execute_input":"2024-07-15T06:23:18.441968Z","iopub.status.busy":"2024-07-15T06:23:18.441691Z","iopub.status.idle":"2024-07-15T06:23:19.642157Z","shell.execute_reply":"2024-07-15T06:23:19.641587Z","shell.execute_reply.started":"2024-07-15T06:23:18.441950Z"},"language":"python","trusted":true},"outputs":[{"data":{"text/plain":"'The Feature-based Approach with BERT involves concatenating the last 4 layers of BERT as features for a given task. In the context provided, this approach was applied to a Named Entity Recognition (NER) task, and it was shown to be the best approach in Section 5.3 of the paper. The right part of the paper shows the Dev set results for this approach.\\n\\nFrom the table, it can be observed that fine-tuning is surprisingly robust to different masking strategies. However, using only the MASK strategy was problematic when applying the feature-based approach to NER. Interestingly, using only the RND strategy performed much worse than the strategy used in the paper.\\n\\nTherefore, the Feature-based Approach with BERT involves using the last 4 layers of BERT as features and fine-tuning these features for a given task. This approach was shown to be robust to different masking strategies, except for the MASK strategy, and it outperformed the RND strategy in a NER task.'"},"execution_count":90,"metadata":{},"output_type":"execute_result"}],"source":"semantic_rag_chain.invoke(\"Describe the Feature-based Approach with BERT?\")"},{"attachments":{},"cell_type":"markdown","id":"eea53ff9-4e1d-456c-9e96-39f514f0614d","metadata":{"language":"python"},"source":"### Ask Question 2"},{"cell_type":"code","execution_count":93,"id":"0ee1eaf2-d6cd-47bd-84e3-db46ee521220","metadata":{"execution":{"iopub.execute_input":"2024-07-15T06:23:36.933570Z","iopub.status.busy":"2024-07-15T06:23:36.933301Z","iopub.status.idle":"2024-07-15T06:23:37.932558Z","shell.execute_reply":"2024-07-15T06:23:37.931632Z","shell.execute_reply.started":"2024-07-15T06:23:36.933548Z"},"language":"python","trusted":true},"outputs":[{"data":{"text/plain":"'SQuAD v2.0, or Squad 2.0, is a version of the Stanford Question Answering Dataset (SQuAD) that extends the problem definition of SQuAD 1.1 by allowing for the possibility that no short answer exists in the provided paragraph. This makes the problem more realistic. To extend the SQuAD v1.1 BERT model for this task, questions that do not have an answer are treated as having an answer span with start and end at the [CLS] token. The probability space for the start and end answer span positions is extended to include the position of the [CLS] token. For prediction, the score of the no-answer span is compared to the score of the best non-null span. The TriviaQA data used for this task consists of paragraphs from TriviaQA-Wiki formed of the first 400 tokens in documents, that contain at least one of the provided possible answers.'"},"execution_count":93,"metadata":{},"output_type":"execute_result"}],"source":"semantic_rag_chain.invoke(\"What is SQuADv2.0?\")"},{"attachments":{},"cell_type":"markdown","id":"616eaab7-feec-4625-9609-fc30bcce84c3","metadata":{"language":"python"},"source":"### Ask Question 3"},{"cell_type":"code","execution_count":94,"id":"d8a03da4-3ed5-4dd1-aaaf-42ce7297e432","metadata":{"execution":{"iopub.execute_input":"2024-07-15T06:24:02.015843Z","iopub.status.busy":"2024-07-15T06:24:02.015567Z","iopub.status.idle":"2024-07-15T06:24:03.068136Z","shell.execute_reply":"2024-07-15T06:24:03.067512Z","shell.execute_reply.started":"2024-07-15T06:24:02.015824Z"},"language":"python","trusted":true},"outputs":[{"data":{"text/plain":"\"Ablation studies are used to understand the impact of different components or settings of a machine learning model on its performance. In the provided context, ablation studies are used to answer specific questions about the BERT model's pre-training process.\\n\\nFor example, one ablation study investigates the effect of the number of training steps on the model's accuracy. By comparing the performance of BERT after pre-training for different numbers of steps, the study finds that BERT BASE achieves higher fine-tuning accuracy when pre-trained for 1 million steps compared to 500k steps.\\n\\nAnother ablation study evaluates the impact of different masking strategies on the model's performance. By comparing the accuracy of BERT when using different masking rates and masking procedures, the study finds that masking 80% of the tokens and randomly selecting 20% of the remaining tokens to be the target tokens results in the highest MNLI and NER accuracy.\\n\\nOverall, ablation studies help to identify which components or settings of a machine learning model are most important for its performance, and can inform decisions about model design and training.\""},"execution_count":94,"metadata":{},"output_type":"execute_result"}],"source":"semantic_rag_chain.invoke(\"What is the purpose of Ablation Studies?\")"},{"attachments":{},"cell_type":"markdown","id":"052a75a0-d2fd-4ca4-abcb-e9e83faeb04b","metadata":{"language":"python"},"source":"## Implement a RAG pipeline using Naive Chunking Strategy"},{"cell_type":"code","execution_count":95,"id":"0f1faa8c-7505-4ac1-a319-51f7843e3259","metadata":{"execution":{"iopub.execute_input":"2024-07-15T06:24:47.822960Z","iopub.status.busy":"2024-07-15T06:24:47.822657Z","iopub.status.idle":"2024-07-15T06:25:30.323639Z","shell.execute_reply":"2024-07-15T06:25:30.323151Z","shell.execute_reply.started":"2024-07-15T06:24:47.822943Z"},"language":"python","trusted":true},"outputs":[],"source":"naive_chunk_vectorstore = SingleStoreDB.from_documents(naive_chunks, embedding=embed_model)\nnaive_chunk_retriever = naive_chunk_vectorstore.as_retriever(search_kwargs={\"k\" : 5})\nnaive_rag_chain = (\n    {\"context\" : naive_chunk_retriever, \"question\" : RunnablePassthrough()}\n    | rag_prompt\n    | chat_model\n    | StrOutputParser()\n)"},{"attachments":{},"cell_type":"markdown","id":"92b3e057-a9aa-4b66-9c04-41e6481d985a","metadata":{"language":"python"},"source":"### Ask Question 1"},{"cell_type":"code","execution_count":96,"id":"09f81609-90b6-4fa4-9e95-8c8046136a3b","metadata":{"execution":{"iopub.execute_input":"2024-07-15T06:25:34.177303Z","iopub.status.busy":"2024-07-15T06:25:34.177052Z","iopub.status.idle":"2024-07-15T06:25:35.281209Z","shell.execute_reply":"2024-07-15T06:25:35.280715Z","shell.execute_reply.started":"2024-07-15T06:25:34.177288Z"},"language":"python","trusted":true},"outputs":[{"data":{"text/plain":"'The Feature-based Approach with BERT involves extracting the activations from one or more layers of the pre-trained BERT model without fine-tuning any of its parameters. These contextual embeddings are then used as input to a separately initialized two-layer 768-dimensional BiLSTM before the classification layer. The results presented in the document show that this method performs competitively with state-of-the-art methods, particularly when concatenating token representations from the top four hidden layers of the pre-trained Transformer. This demonstrates the effectiveness of BERT for both fine-tuning and feature-based approaches. In the context provided, the feature-based approach uses the last four layers of BERT, which was found to be the best approach in Section 5.3. Fine-tuning, on the other hand, is shown to be robust to different masking strategies during MLM pre-training.'"},"execution_count":96,"metadata":{},"output_type":"execute_result"}],"source":"naive_rag_chain.invoke(\"Describe the Feature-based Approach with BERT?\")"},{"attachments":{},"cell_type":"markdown","id":"aeee776c-c599-4f7e-9c7e-5f51943a02c5","metadata":{"language":"python"},"source":"### Ask Question 2"},{"cell_type":"code","execution_count":97,"id":"a0b892b3-b79b-4624-9298-9e41d4053158","metadata":{"execution":{"iopub.execute_input":"2024-07-15T06:25:50.800261Z","iopub.status.busy":"2024-07-15T06:25:50.800015Z","iopub.status.idle":"2024-07-15T06:25:51.830962Z","shell.execute_reply":"2024-07-15T06:25:51.830482Z","shell.execute_reply.started":"2024-07-15T06:25:50.800246Z"},"language":"python","trusted":true},"outputs":[{"data":{"text/plain":"'SQuAD 2.0 is a version of the Stanford Question Answering Dataset (SQuAD) that extends the problem definition of SQuAD 1.1 by allowing for the possibility that no short answer exists in the provided paragraph. This makes the problem more realistic. To handle this, the SQuAD v1.1 BERT model is extended to treat questions that do not have an answer as having an answer span with start and end at the [CLS] token, and the probability space for the start and end answer span positions is extended to include the position of the [CLS] token. The score of the no-answer span is then compared to the score of the best non-null span for prediction. The TriviaQA data used consists of paragraphs from TriviaQA-Wiki formed of the first 400 tokens in documents, that contain at least one of the provided possible answers.'"},"execution_count":97,"metadata":{},"output_type":"execute_result"}],"source":"naive_rag_chain.invoke(\"What is SQuADv2.0?\")"},{"attachments":{},"cell_type":"markdown","id":"af4e7a41-4032-461e-be98-97407e2f4254","metadata":{"language":"python"},"source":"### Ask Question 3"},{"cell_type":"code","execution_count":98,"id":"39a16270-7ace-4382-9bf2-c8a97fac672c","metadata":{"execution":{"iopub.execute_input":"2024-07-15T06:26:04.077072Z","iopub.status.busy":"2024-07-15T06:26:04.076753Z","iopub.status.idle":"2024-07-15T06:26:05.163345Z","shell.execute_reply":"2024-07-15T06:26:05.162861Z","shell.execute_reply.started":"2024-07-15T06:26:04.077053Z"},"language":"python","trusted":true},"outputs":[{"data":{"text/plain":"'Ablation studies are used to evaluate the effect of different components or settings in a machine learning model. In the provided context, there are two ablation studies mentioned:\\n\\n1. Effect of Number of Training Steps: This study investigates the impact of the number of training steps on the performance of BERT. By comparing the MNLI Dev accuracy of BERT BASE after fine-tuning from a checkpoint pre-trained for different numbers of steps, the study aims to answer questions about the necessity and convergence rate of MLM pre-training.\\n\\n2. Ablation for Different Masking Procedures: This study evaluates the effect of different masking strategies used during pre-training with the masked language model (MLM) objective. By comparing the MNLI Dev accuracy and NER Dev set results of BERT BASE using different masking rates and approaches, the study aims to reduce the mismatch between pre-training and fine-tuning.\\n\\nIn summary, ablation studies help to understand the importance and impact of different components or settings in a model, providing insights for improving model performance and design.'"},"execution_count":98,"metadata":{},"output_type":"execute_result"}],"source":"naive_rag_chain.invoke(\"What is the purpose of Ablation Studies?\")"},{"cell_type":"code","execution_count":null,"id":"cca1aa70-0c4f-493f-acf0-b9c9c31bc656","metadata":{"language":"python","trusted":true},"outputs":[],"source":""}],"metadata":{"jupyterlab":{"notebooks":{"version_major":6,"version_minor":4}},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"},"singlestore_cell_default_language":"python","singlestore_connection":{"connectionID":"77746612-3eb9-447b-b400-efdec97e2064","defaultDatabase":"db_vector"},"singlestore_row_limit":300},"nbformat":4,"nbformat_minor":5}